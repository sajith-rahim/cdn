source('~/PMRL-R-Workspace/Linear Basis Models.R')
source('~/PMRL-R-Workspace/Linear Basis Models.R')
source('~/PMRL-R-Workspace/Linear Basis Models.R')
source('~/PMRL-R-Workspace/Linear Basis Models.R')
source('~/PMRL-R-Workspace/Linear Basis Models.R')
source('~/PMRL-R-Workspace/Linear Basis Models.R')
source('~/PMRL-R-Workspace/Linear Basis Models.R')
# basis for quadratic regression
phi <- c(one, id, sq)
W <- compute_w(X, Y, phi)
plot(X,Y,pch=19)
draw_regression(X,W,phi)
draw_regression(X,W,phi)
phi <- c(one, id, function(x) sin(x))
W <- compute_w(X, Y, phi)
plot(X,Y,pch=19)
draw_regression(X,W,phi)
?plot
?abline
source('~/PMRL-R-Workspace/Linear Basis Models-Regularization.R')
source('~/PMRL-R-Workspace/Linear Basis Models-Regularization.R')
?plot
source('~/PMRL-R-Workspace/Linear Basis Models-Regularization.R')
source('~/PMRL-R-Workspace/Linear Basis Models-Regularization.R')
#----------------------------------------------------------
#----------------Linear Basis Models Sequential Learning---
source('~/PMRL-R-Workspace/Linear Basis Models-Sequential.R')
source('~/PMRL-R-Workspace/Linear Basis Models-Sequential.R')
source('~/PMRL-R-Workspace/Linear Basis Models-Sequential.R')
source('~/PMRL-R-Workspace/Chapter 11/Metropolis Algorithm.R')
hist(res, 50, freq=FALSE, main="", ylim=c(0, .4), las=1,
xlab="x", ylab="Probability density")
z <- integrate(f, -Inf, Inf)$value
curve(f(x) / z, add=TRUE, col="red", n=200)
set.seed(1)
res.long <- run(-10, f, q, 50000)
hist(res.long, 100, freq=FALSE, main="", ylim=c(0, .4), las=1,
xlab="x", ylab="Probability density", col="grey")
z <- integrate(f, -Inf, Inf)$value
curve(f(x) / z, add=TRUE, col="red", n=200)
curve(f(x) / z, add=F, col="red", n=200)
set.seed(1)
res.long <- run(-10, f, q, 50000)
hist(res.long, 100, freq=FALSE, main="", ylim=c(0, .4), las=1,
xlab="x", ylab="Probability density", col="grey")
z <- integrate(f, -Inf, Inf)$value
curve(f(x) / z, add=TRUE, col="red", n=200)
set.seed(1)
res.long <- run(-10, f, q, 50000)
hist(res.long, 100, freq=FALSE, main="", ylim=c(0, .4), las=1,
xlab="x", ylab="Probability density", col="grey")
z <- integrate(f, -Inf, Inf)$value
curve(f(x) / z, add=TRUE, col="red", n=200)
par(mfrow=c(2,2), mar=rep(.5, 4), oma=c(4, 4, 0, 0))
for (h in hh) {
plot(h, main="", freq=FALSE, yaxt="n",
ylim=range(h$density, ylim))
curve(f(x), add=TRUE, col="red", n=300)
}
#100, 1,000, 10,000 and 100,000 steps
n <- 10^(2:5)
samples <- lapply(n, function(n) run(-10, f, q, n))
xlim <- range(sapply(samples, range))
br <- seq(xlim[1], xlim[2], length=100)
hh <- lapply(samples, function(x) hist(x, br, plot=FALSE))
ylim <- c(0, max(f(xx)))
par(mfrow=c(2,2), mar=rep(.5, 4), oma=c(4, 4, 0, 0))
for (h in hh) {
plot(h, main="", freq=FALSE, yaxt="n",
ylim=range(h$density, ylim))
curve(f(x), add=TRUE, col="red", n=300)
}
q <- function(x, d=8)
x + runif(length(x), -d/2, d/2)
q(c(1,2))
q <- function(x, d=8)
x + runif(length(x), -d/2, d/2)
1(1,1)
q(c(1,2),1)
make.mvn <- function(mean, vcv) {
logdet <- as.numeric(determinant(vcv, TRUE)$modulus)
tmp <- length(mean) * log(2 * pi) + logdet
vcv.i <- solve(vcv)
function(x) {
dx <- x - mean
exp(-(tmp + rowSums((dx %*% vcv.i) * dx))/2)
}
}
#Mixture Unweigted
mu1 <- c(-1, 1)
mu2 <- c(2, -2)
vcv1 <- matrix(c(1, .25, .25, 1.5), 2, 2)
vcv2 <- matrix(c(2, -.5, -.5, 2), 2, 2)
f1 <- make.mvn(mu1, vcv1)
f2 <- make.mvn(mu2, vcv2)
f <- function(x)
f1(x) + f2(x)
x <- seq(-5, 6, length=71)
y <- seq(-7, 6, length=61)
xy <- expand.grid(x=x, y=y)
z <- matrix(apply(as.matrix(xy), 1, f), length(x), length(y))
image(x, y, z, las=1)
contour(x, y, z, add=TRUE)
q <- function(x, d=8)
x + runif(length(x), -d/2, d/2)
x0 <- c(-4, -4)
set.seed(1)
samples <- run(x0, f, q, 1000)
image(x, y, z, xlim=range(x, samples[,1]), ylim=range(x, samples[,2]))
contour(x, y, z, add=TRUE)
lines(samples[,1], samples[,2], col="#00000088")
source('~/PMRL-R-Workspace/Chapter 11/Metropolis Algorithm.R')
image(x, y, z, xlim=range(x, samples[,1]), ylim=range(x, samples[,2]))
contour(x, y, z, add=TRUE)
lines(samples[,1], samples[,2], col="#00000088")
#100000 samples
samples <- run(x0, f, q, 100000)
smoothScatter(samples)
contour(x, y, z, add=TRUE)
smoothScatter(samples)
#100000 samples
samples <- run(x0, f, q, 10000000)
smoothScatter(samples)
#---------------Importance Sampling-------------------
i.sampling <- function(f, g, h, rh, n=1e4) {
ys <- rh(n)
mean(g(ys)*f(ys)/h(ys))
}
#P(X>4.5)
xs <- seq(0.1,10,by=0.05)
plot(xs,dexp(xs),col="blue", type="l")   # the exponential pdf
lines(xs,exp(-(xs-4.5)),col="red",lwd=2) # the truncated pdf
abline(v=4.5,lty=2)
n.accepts     <- 0
source('~/PMRL-R-Workspace/Chapter 11/Rejection Sampling.R')
debugSource('~/PMRL-R-Workspace/Chapter 11/Metropolis Algorithm.R')
res
shape(res)
str(res)
res[i,] <- x <- step(x, f, q)
drop(res)
run <- function(x, f, q, nsteps) {
res <- matrix(NA, nsteps, length(x))
for (i in seq_len(nsteps))
res[i,] <- x <- step(x, f, q)
drop(res)
}
res <- run(-10, f, q, 1000)
layout(matrix(c(1, 2), 1, 2), widths=c(4, 1))
par(mar=c(4.1, .5, .5, .5), oma=c(0, 4.1, 0, 0))
plot(res, type="s", xpd=NA, ylab="Parameter", xlab="Sample", las=1)
usr <- par("usr")
x
debugSource('~/PMRL-R-Workspace/Chapter 11/Metropolis Algorithm.R')
setwd("~/PMRL-R-Workspace/backprop")
source('~/PMRL-R-Workspace/backprop/backprop.R')
X
cbind(X, y)
X <- matrix(c(
0,0,1,
0,1,1,
1,0,1,
1,1,1
),
ncol = 3,
byrow = TRUE
)
y <- c(0, 1, 1, 0)
cbind(X, y)
# initial weights
rand_vector <- runif(ncol(X) * nrow(X))
rand_matrix <- matrix(
rand_vector,
nrow = ncol(X),
ncol = nrow(X),
byrow = TRUE
)
# nn object
my_nn <- list(
# imput
input = X,
# w-layer1
weights1 = rand_matrix,
# w-layer2
weights2 = matrix(runif(4), ncol = 1),
# actual observed
y = y,
# output
output = matrix(
rep(0, times = 4),
ncol = 1
)
)
rand_matrix
X <- matrix(c(
0,0,1,
0,1,1,
1,0,1,
1,1,1
),
ncol = 3,
byrow = TRUE
)
y <- c(0, 1, 1, 0)
cbind(X, y)
# initial weights
rand_vector <- runif(ncol(X) * nrow(X))
rand_matrix <- matrix(
rand_vector,
nrow = ncol(X),
ncol = nrow(X),
byrow = TRUE
)
# nn object
my_nn <- list(
# imput
input = X,
# w-layer1
weights1 = rand_matrix,
# w-layer2
weights2 = matrix(runif(4), ncol = 1),
# actual observed
y = y,
# output
output = matrix(
rep(0, times = 4),
ncol = 1
)
)
#sigmoid activation function
sigmoid <- function(x) {
1.0 / (1.0 + exp(-x))
}
#derivative of the activation function
sigmoid_derivative <- function(x) {
x * (1.0 - x)
}
# (t-o)^2
loss_function <- function(nn) {
sum((nn$y - nn$output) ^ 2)
}
feedforward <- function(nn) {
nn$layer1 <- sigmoid(nn$input %*% nn$weights1)
nn$output <- sigmoid(nn$layer1 %*% nn$weights2)
nn
}
backprop <- function(nn) {
# application of the chain rule to find derivative of the loss function with
# respect to weights2 and weights1
d_weights2 <- (
t(nn$layer1) %*%
# `2 * (nn$y - nn$output)` is the derivative of the sigmoid loss function
(2 * (nn$y - nn$output) *
sigmoid_derivative(nn$output))
)
d_weights1 <- ( 2 * (nn$y - nn$output) * sigmoid_derivative(nn$output)) %*%
t(nn$weights2)
d_weights1 <- d_weights1 * sigmoid_derivative(nn$layer1)
d_weights1 <- t(nn$input) %*% d_weights1
# update the weights using the derivative (slope) of the loss function
nn$weights1 <- nn$weights1 + d_weights1
nn$weights2 <- nn$weights2 + d_weights2
nn
}
# number of times to perform feedforward and backpropagation
n <- 1500
# data frame to store the results of the loss function.
loss_df <- data.frame(
iteration = 1:n,
loss = vector("numeric", length = n)
)
for (i in seq_len(1500)) {
my_nn <- feedforward(my_nn)
my_nn <- backprop(my_nn)
# store the result of the loss function to plot later
loss_df$loss[i] <- loss_function(my_nn)
}
# print the predicted outcome next to the actual outcome
data.frame(
"Predicted" = round(my_nn$output, 3),
"Actual" = y
)
X <- matrix(c(
0,0,1,
0,1,1,
1,0,1,
1,1,1
),
ncol = 3,
byrow = TRUE
)
y <- c(0, 1, 1, 0)
cbind(X, y)
# initial weights
rand_vector <- runif(ncol(X) * nrow(X))
rand_matrix <- matrix(
rand_vector,
nrow = ncol(X),
ncol = nrow(X),
byrow = TRUE
)
# nn object
my_nn <- list(
# imput
input = X,
# w-layer1
weights1 = rand_matrix,
# w-layer2
weights2 = matrix(runif(4), ncol = 1),
# actual observed
y = y,
# output
output = matrix(
rep(0, times = 4),
ncol = 1
)
)
#sigmoid activation function
sigmoid <- function(x) {
1.0 / (1.0 + exp(-x))
}
#derivative of the activation function
sigmoid_derivative <- function(x) {
x * (1.0 - x)
}
# (t-o)^2
loss_function <- function(nn) {
sum((nn$y - nn$output) ^ 2)
}
feedforward <- function(nn) {
nn$layer1 <- sigmoid(nn$input %*% nn$weights1)
nn$output <- sigmoid(nn$layer1 %*% nn$weights2)
nn
}
backprop <- function(nn) {
# application of the chain rule to find derivative of the loss function with
# respect to weights2 and weights1
d_weights2 <- (
t(nn$layer1) %*%
# `2 * (nn$y - nn$output)` is the derivative of the sigmoid loss function
(2 * (nn$y - nn$output) *
sigmoid_derivative(nn$output))
)
d_weights1 <- ( 2 * (nn$y - nn$output) * sigmoid_derivative(nn$output)) %*%
t(nn$weights2)
d_weights1 <- d_weights1 * sigmoid_derivative(nn$layer1)
d_weights1 <- t(nn$input) %*% d_weights1
# update the weights using the derivative (slope) of the loss function
nn$weights1 <- nn$weights1 + d_weights1
nn$weights2 <- nn$weights2 + d_weights2
nn
}
# number of times to perform feedforward and backpropagation
n <- 1500
# data frame to store the results of the loss function.
loss_df <- data.frame(
iteration = 1:n,
loss = vector("numeric", length = n)
)
for (i in seq_len(1500)) {
my_nn <- feedforward(my_nn)
my_nn <- backprop(my_nn)
# store the result of the loss function to plot later
loss_df$loss[i] <- loss_function(my_nn)
}
# print the predicted outcome next to the actual outcome
data.frame(
"Predicted" = round(my_nn$output, 3),
"Actual" = y
)
ggplot(data = loss_df, aes(x = iteration, y = loss)) +
geom_line()
install.packages("ggplot")
install.packages("ggplot2")
library(ggplot2)
# plot the cost
library(ggplot2)
ggplot(data = loss_df, aes(x = iteration, y = loss)) +
geom_line()
View(rand_matrix)
View(rand_matrix)
View(my_nn)
View(my_nn)
my_nn <- list(
# imput
input = X,
# w-layer1
weights1 = rand_matrix,
# w-layer2
weights2 = matrix(runif(4), ncol = 1),
# actual observed
y = y,
# output
output = matrix(
rep(0, times = 4),
ncol = 1
)
)
View(my_nn)
View(my_nn)
my_nn[["input"]]
my_nn[["input"]]
my_nn[["weights1"]]
my_nn[["weights2"]]
my_nn[["y"]]
debugSource('~/PMRL-R-Workspace/backprop/backprop.R')
force(nn)
View(nn)
sigmoid_derivative(nn$output)
t(nn$layer1)
t(nn$layer1) %*%
# `2 * (nn$y - nn$output)` is the derivative of the sigmoid loss function
(2 * (nn$y - nn$output) *
sigmoid_derivative(nn$output))
t(nn$layer1) %*% (2 * (nn$y - nn$output) *
sigmoid_derivative(nn$output))
# respect to weights2 and weights1
source('~/PMRL-R-Workspace/backprop/backprop.R')
source('~/PMRL-R-Workspace/backprop/backprop.R')
source('~/PMRL-R-Workspace/backprop/backprop.R')
X <- matrix(c(
0,0,1,
0,1,1,
1,0,1,
1,1,1
),
ncol = 3,
byrow = TRUE
)
y <- c(0, 1, 1, 0)
cbind(X, y)
# initial weights
rand_vector <- runif(ncol(X) * nrow(X))
rand_matrix <- matrix(
rand_vector,
nrow = ncol(X),
ncol = nrow(X),
byrow = TRUE
)
# nn object
my_nn <- list(
# imput
input = X,
# w-layer1
weights1 = rand_matrix,
# w-layer2
weights2 = matrix(runif(4), ncol = 1),
# actual observed
y = y,
# output
output = matrix(
rep(0, times = 4),
ncol = 1
)
)
#sigmoid activation function
sigmoid <- function(x) {
1.0 / (1.0 + exp(-x))
}
#derivative of the activation function
sigmoid_derivative <- function(x) {
x * (1.0 - x)
}
# (t-o)^2
loss_function <- function(nn) {
sum((nn$y - nn$output) ^ 2)
}
feedforward <- function(nn) {
nn$layer1 <- sigmoid(nn$input %*% nn$weights1)
nn$output <- sigmoid(nn$layer1 %*% nn$weights2)
nn
}
backprop <- function(nn) {
# application of the chain rule to find derivative of the loss function with
# respect to weights2 and weights1
d_weights2 <- (
t(nn$layer1) %*%
# `2 * (nn$y - nn$output)` is the derivative of the sigmoid loss function
(2 * (nn$y - nn$output) *
sigmoid_derivative(nn$output))
)
d_weights1 <- ( 2 * (nn$y - nn$output) * sigmoid_derivative(nn$output)) %*%
t(nn$weights2)
d_weights1 <- d_weights1 * sigmoid_derivative(nn$layer1)
d_weights1 <- t(nn$input) %*% d_weights1
# update the weights using the derivative (slope) of the loss function
nn$weights1 <- nn$weights1 + d_weights1
nn$weights2 <- nn$weights2 + d_weights2
nn
}
# number of times to perform feedforward and backpropagation
n <- 1500
# data frame to store the results of the loss function.
loss_df <- data.frame(
iteration = 1:n,
loss = vector("numeric", length = n)
)
for (i in seq_len(1500)) {
my_nn <- feedforward(my_nn)
my_nn <- backprop(my_nn)
# store the result of the loss function to plot later
loss_df$loss[i] <- loss_function(my_nn)
}
# print the predicted outcome next to the actual outcome
data.frame(
"Predicted" = round(my_nn$output, 3),
"Actual" = y
)
# plot the cost
library(ggplot2)
ggplot(data = loss_df, aes(x = iteration, y = loss)) +
geom_line()
